"""URL æŠ“å–æ¨¡å— - æ”¯æŒæå–ç½‘é¡µæ­£æ–‡"""
import re
import base64
import random
from typing import Optional, Tuple
from urllib.parse import urljoin

import requests

# è®°å½•æµè§ˆå™¨æ˜¯å¦å·²ç»ä»¥è°ƒè¯•æ¨¡å¼é‡å¯è¿‡
_browser_restarted = False

# ç®€åŒ–ç‰ˆç¬”ç”»æ•°æ®ï¼šç›¸å¯¹åæ ‡ç‚¹
STROKES = {
    "æ¨ª": [(0, 0), (0.3, 0.02), (0.6, -0.01), (1, 0)],
    "ç«–": [(0, 0), (0.02, 0.3), (-0.01, 0.6), (0, 1)],
    "æ’‡": [(0, 0), (-0.2, 0.3), (-0.4, 0.6), (-0.5, 1)],
    "æº": [(0, 0), (0.2, 0.3), (0.4, 0.6), (0.5, 1)],
    "ç‚¹": [(0, 0), (0.1, 0.2), (0.15, 0.4)],
}

DEBUG_STROKE = False  # è®¾ä¸º True å¯åœ¨é¡µé¢ä¸Šçœ‹åˆ°ç¬”ç”»è½¨è¿¹


def _simulate_stroke(page):
    """åœ¨é¡µé¢ä¸Šæ¨¡æ‹Ÿä¸€ä¸¤ç¬”é¼ æ ‡ç§»åŠ¨"""
    import time
    stroke_name = random.choice(list(STROKES.keys()))
    points = STROKES[stroke_name]

    start_x = random.randint(200, 800)
    start_y = random.randint(200, 500)
    scale = random.randint(30, 80)

    js_points = []
    for px, py in points:
        x = start_x + px * scale + random.uniform(-2, 2)
        y = start_y + py * scale + random.uniform(-2, 2)
        js_points.append(f"[{x:.1f}, {y:.1f}]")

    debug_js = ""
    if DEBUG_STROKE:
        debug_js = f'''
            const canvas = document.createElement('canvas');
            canvas.style.cssText = 'position:fixed;top:0;left:0;width:100%;height:100%;pointer-events:none;z-index:99999';
            canvas.width = window.innerWidth;
            canvas.height = window.innerHeight;
            document.body.appendChild(canvas);
            const ctx = canvas.getContext('2d');
            ctx.strokeStyle = 'red';
            ctx.lineWidth = 3;
            ctx.beginPath();
            ctx.moveTo(points[0][0], points[0][1]);
            points.forEach(p => ctx.lineTo(p[0], p[1]));
            ctx.stroke();
            ctx.fillStyle = 'red';
            ctx.font = '16px sans-serif';
            ctx.fillText('ç¬”ç”»: {stroke_name}', points[0][0], points[0][1] - 10);
            setTimeout(() => canvas.remove(), 2000);
        '''
        print(f"[DEBUG] æ¨¡æ‹Ÿç¬”ç”»: {stroke_name}, èµ·ç‚¹: ({start_x}, {start_y})")

    page.run_js(f'''
        const points = [{", ".join(js_points)}];
        {debug_js}
        let i = 0;
        const interval = setInterval(() => {{
            if (i >= points.length) {{ clearInterval(interval); return; }}
            const [x, y] = points[i++];
            document.elementFromPoint(x, y)?.dispatchEvent(
                new MouseEvent('mousemove', {{clientX: x, clientY: y, bubbles: true}})
            );
        }}, {random.randint(30, 60)});
    ''')
    time.sleep(0.2)


def _extract_page_content(page, embed_images: bool, save_path: str = None, save_callback=None) -> Tuple[str, str]:
    """ä»é¡µé¢æå–å†…å®¹å’Œå›¾ç‰‡"""
    import time
    import random

    def save_snapshot():
        """ä¿å­˜å½“å‰é¡µé¢å¿«ç…§"""
        if not save_callback:
            return
        try:
            content = page.html
            title = page.title
            save_callback(content, title)
        except Exception:
            pass

    # æ¬ºéª—é¡µé¢å¯è§æ€§æ£€æµ‹ï¼Œé˜²æ­¢æœ€å°åŒ–/åˆ‡æ¢åº”ç”¨æ—¶æ‡’åŠ è½½åœæ­¢
    page.run_js('''
        Object.defineProperty(document, 'hidden', { get: () => false });
        Object.defineProperty(document, 'visibilityState', { get: () => 'visible' });
        document.addEventListener('visibilitychange', e => e.stopImmediatePropagation(), true);
        // æ¬ºéª— IntersectionObserverï¼Œè®©æ‰€æœ‰å…ƒç´ éƒ½è¢«è®¤ä¸ºå¯è§
        if (window.IntersectionObserver) {
            window.IntersectionObserver = class {
                constructor(cb) { this.cb = cb; }
                observe(el) { this.cb([{ isIntersecting: true, target: el }]); }
                unobserve() {}
                disconnect() {}
            };
        }
    ''')

    initial_height = page.run_js('return document.body.scrollHeight')

    # åˆ†æ®µæ»šåŠ¨åˆ°åº•éƒ¨
    scroll_pos = 0
    while scroll_pos < page.run_js('return document.body.scrollHeight'):
        scroll_pos += 1600
        page.run_js(f'window.scrollTo(0, {scroll_pos})')
        time.sleep(round(random.uniform(0.1, 0.4), 3))

        # éšæœºç”»ä¸€ç¬”ï¼ˆ30%æ¦‚ç‡ï¼‰ï¼Œç”»çš„æ—¶å€™åœä¸‹æ¥
        if random.random() < 0.3:
            time.sleep(0.3)
            _simulate_stroke(page)
            time.sleep(0.5)

    # ç¬¬ä¸€è½®æ»šåŠ¨åç­‰å¾…ï¼Œè®©æ‡’åŠ è½½æœ‰æ—¶é—´è§¦å‘
    time.sleep(1.0)
    save_snapshot()  # ç¬¬ä¸€è½®æ»šåŠ¨åä¿å­˜
    final_height = page.run_js('return document.body.scrollHeight')

    is_lazy = final_height > initial_height

    if is_lazy:
        # æ‡’åŠ è½½é¡µé¢ï¼Œç»§ç»­æ»šåŠ¨ç›´åˆ°æ²¡æœ‰æ–°å†…å®¹
        no_change_count = 0
        while no_change_count < 3:
            last_height = page.run_js('return document.body.scrollHeight')
            scroll_pos = page.run_js('return window.scrollY')
            while scroll_pos < last_height:
                scroll_pos += 1600
                page.run_js(f'window.scrollTo(0, {scroll_pos})')
                time.sleep(round(random.uniform(0.1, 0.4), 3))

                # éšæœºç”»ä¸€ç¬”ï¼ˆ30%æ¦‚ç‡ï¼‰
                if random.random() < 0.3:
                    time.sleep(0.3)
                    _simulate_stroke(page)
                    time.sleep(0.5)

            # åœ¨åº•éƒ¨åœé¡¿æ›´ä¹…ï¼Œç­‰å¾…æ‡’åŠ è½½
            time.sleep(1.0)
            save_snapshot()  # æ¯è½®æ»šåŠ¨åä¿å­˜
            new_height = page.run_js('return document.body.scrollHeight')
            if new_height == last_height:
                no_change_count += 1
            else:
                no_change_count = 0

    page.run_js('window.scrollTo(0, 0)')
    time.sleep(0.5)

    img_cache = {}
    if embed_images:
        for img in page.eles('tag:img'):
            src = img.attr('src')
            if not src or src.startswith('data:'):
                continue
            try:
                b64 = page.run_js('''
                    var img = arguments[0];
                    var canvas = document.createElement('canvas');
                    canvas.width = img.naturalWidth || img.width;
                    canvas.height = img.naturalHeight || img.height;
                    var ctx = canvas.getContext('2d');
                    ctx.drawImage(img, 0, 0);
                    try { return canvas.toDataURL(); } catch(e) { return null; }
                ''', img)
                if b64:
                    img_cache[src] = b64
            except Exception:
                pass

    content = page.html
    title = page.title

    if img_cache:
        for src, b64 in img_cache.items():
            content = content.replace(f'src="{src}"', f'src="{b64}"')
            content = content.replace(f"src='{src}'", f"src='{b64}'")

    return content, title


def _fetch_with_browser(url: str, timeout: float, embed_images: bool = True, save_path: str = None, save_callback=None) -> Tuple[str, str]:
    """ç”¨ DrissionPage æ‰“å¼€é¡µé¢å¹¶è‡ªåŠ¨è·å–å†…å®¹ï¼ˆä½¿ç”¨ç³»ç»Ÿé»˜è®¤æµè§ˆå™¨ï¼‰"""
    from DrissionPage import ChromiumPage, ChromiumOptions
    import socket
    import subprocess
    import os
    import time

    def port_in_use(port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(('127.0.0.1', port)) == 0

    # å°è¯•è¿æ¥å·²æœ‰è°ƒè¯•ç«¯å£çš„æµè§ˆå™¨
    for debug_port in [9222, 9223, 9224]:
        if port_in_use(debug_port):
            try:
                co = ChromiumOptions()
                co.set_local_port(debug_port)
                page = ChromiumPage(co)
                # æ–°å¼€æ ‡ç­¾é¡µ
                tab = page.new_tab(url)
                tab.wait.doc_loaded(timeout=timeout)
                result = _extract_page_content(tab, embed_images, save_path, save_callback)
                tab.close()
                return result
            except Exception:
                pass

    # è·å–é»˜è®¤æµè§ˆå™¨è·¯å¾„
    import winreg
    browser_path = None
    user_data = None
    try:
        with winreg.OpenKey(winreg.HKEY_CURRENT_USER,
            r"Software\Microsoft\Windows\Shell\Associations\UrlAssociations\http\UserChoice") as key:
            prog_id = winreg.QueryValueEx(key, "ProgId")[0]
        with winreg.OpenKey(winreg.HKEY_CLASSES_ROOT, rf"{prog_id}\shell\open\command") as key:
            cmd = winreg.QueryValueEx(key, "")[0]
            browser_path = cmd.split('"')[1] if cmd.startswith('"') else cmd.split()[0]
        # ç¡®å®šç”¨æˆ·æ•°æ®ç›®å½•
        if 'Edge Dev' in browser_path:
            user_data = os.path.expandvars(r'%LOCALAPPDATA%\Microsoft\Edge Dev\User Data')
        elif 'Edge' in browser_path:
            user_data = os.path.expandvars(r'%LOCALAPPDATA%\Microsoft\Edge\User Data')
        elif 'Chrome' in browser_path:
            user_data = os.path.expandvars(r'%LOCALAPPDATA%\Google\Chrome\User Data')
    except Exception:
        pass

    # æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦åœ¨è¿è¡Œï¼ˆæ— è°ƒè¯•ç«¯å£ï¼‰
    global _browser_restarted
    browser_name = os.path.basename(browser_path) if browser_path else 'msedge.exe'
    result = subprocess.run(['tasklist', '/FI', f'IMAGENAME eq {browser_name}'], capture_output=True, text=True)
    browser_running = browser_name.lower() in result.stdout.lower()

    if browser_running and not _browser_restarted:
        # å¼¹çª—è¯¢é—®ç”¨æˆ·
        import ctypes
        msg = "æœªæ£€æµ‹åˆ°è°ƒè¯•ç«¯å£ã€‚\n\næ˜¯å¦å…³é—­å½“å‰æµè§ˆå™¨å¹¶ä»¥è°ƒè¯•æ¨¡å¼é‡å¯ï¼Ÿ\nï¼ˆè¿™æ ·å¯ä»¥ä¿ç•™ä½ çš„ç™»å½•çŠ¶æ€ï¼‰"
        ret = ctypes.windll.user32.MessageBoxW(0, msg, "éœ€è¦é‡å¯æµè§ˆå™¨", 0x31)  # MB_OKCANCEL | MB_ICONWARNING
        if ret != 1:  # ç”¨æˆ·å–æ¶ˆ
            raise Exception("ç”¨æˆ·å–æ¶ˆæ“ä½œ")

        # å…³é—­æµè§ˆå™¨
        subprocess.run(['taskkill', '/F', '/IM', browser_name], capture_output=True)
        time.sleep(1)
        _browser_restarted = True

    # ä»¥è°ƒè¯•æ¨¡å¼å¯åŠ¨æµè§ˆå™¨
    if browser_path and user_data:
        if not port_in_use(9222):
            cmd = [browser_path, '--remote-debugging-port=9222', f'--user-data-dir={user_data}', '--restore-last-session']
            subprocess.Popen(cmd)
            time.sleep(1)

        # è¿æ¥åˆ°æ–°å¯åŠ¨çš„æµè§ˆå™¨
        co = ChromiumOptions()
        co.set_local_port(9222)
        page = ChromiumPage(co)
        tab = page.new_tab(url)
        tab.wait.doc_loaded(timeout=timeout)
        result = _extract_page_content(tab, embed_images, save_path, save_callback)
        tab.close()
        return result

    # å›é€€ï¼šå¯åŠ¨æ–°å®ä¾‹ï¼ˆæ— cookiesï¼‰
    co = ChromiumOptions()
    if browser_path:
        co.set_browser_path(browser_path)
    port = 19222
    while port_in_use(port):
        port += 1
    co.set_local_port(port)
    page = ChromiumPage(co)
    page.get(url)
    page.wait.doc_loaded(timeout=timeout)
    return _extract_page_content(page, embed_images, save_path, save_callback)


def fetch_url(
    url: str,
    timeout: float = 15.0,
    proxy: Optional[str] = None,
    main_only: bool = False,
    download_images: bool = True,
    cookie: Optional[str] = None,
    use_browser: bool = False,
    save_path: Optional[str] = None,
    save_callback=None
) -> Tuple[str, str]:
    """
    æŠ“å– URL å†…å®¹
    è¿”å›: (html_content, title)
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    }
    if cookie:
        headers['Cookie'] = cookie
    proxies = {"http": proxy, "https": proxy} if proxy else None

    if use_browser:
        content, title = _fetch_with_browser(url, timeout, embed_images=download_images, save_path=save_path, save_callback=save_callback)
    else:
        resp = requests.get(url, timeout=timeout, proxies=proxies, headers=headers)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding or 'utf-8'
        content = resp.text
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
        title = title_match.group(1).strip() if title_match else url

    if main_only:
        # çŸ¥ä¹ç‰¹æ®Šå¤„ç†
        if 'zhihu.com' in url:
            if use_browser:
                zhihu_content = _extract_zhihu_from_dom(content)
            else:
                zhihu_content = _extract_zhihu_content(content)
            if zhihu_content:
                content = zhihu_content
            else:
                content = _simple_extract(content)
        # CSDNç‰¹æ®Šå¤„ç†
        elif 'csdn.net' in url:
            content = _extract_csdn_content(content)
        # å¾®åšç‰¹æ®Šå¤„ç†
        elif 'weibo.com' in url or 'weibo.cn' in url:
            content = _extract_weibo_content(content)
        else:
            content = _simple_extract(content)
        # è¿‡æ»¤å™ªéŸ³å›¾ç‰‡
        content = _filter_noise_images(content)

    # ä¸‹è½½å›¾ç‰‡å¹¶è½¬ä¸ºbase64ï¼ˆéæµè§ˆå™¨æ¨¡å¼ï¼‰
    if download_images and not use_browser:
        content = _embed_images(content, url, headers, proxies, timeout)

    return content, title


def _embed_images(html: str, base_url: str, headers: dict, proxies: dict, timeout: float) -> str:
    """ä¸‹è½½å›¾ç‰‡å¹¶è½¬ä¸ºbase64å†…åµŒ"""
    def replace_img(match):
        img_url = match.group(2)
        if img_url.startswith('data:'):
            return match.group(0)
        full_url = urljoin(base_url, img_url)
        try:
            img_headers = {**headers, 'Referer': base_url}
            resp = requests.get(full_url, timeout=timeout, proxies=proxies, headers=img_headers)
            if resp.status_code == 200:
                content_type = resp.headers.get('Content-Type', 'image/jpeg')
                if 'image' in content_type:
                    b64 = base64.b64encode(resp.content).decode('utf-8')
                    return f'{match.group(1)}data:{content_type};base64,{b64}{match.group(3)}'
        except Exception:
            pass
        return match.group(0)

    # åŒ¹é… src="..." æˆ– src='...'
    pattern = r'(<img[^>]*\ssrc=["\'])([^"\']+)(["\'][^>]*>)'
    return re.sub(pattern, replace_img, html, flags=re.IGNORECASE)


def _extract_csdn_content(html: str) -> str:
    """ä»CSDNé¡µé¢æå–æ­£æ–‡"""
    # æå–æ–‡ç« ä¸»ä½“ (article_content æˆ– content_views)
    match = re.search(r'<article[^>]*class="[^"]*baidu_pl[^"]*"[^>]*>([\s\S]+?)</article>', html, re.IGNORECASE)
    if not match:
        match = re.search(r'<div[^>]*id="content_views"[^>]*>([\s\S]+?)</div>\s*<div[^>]*class="[^"]*hide-article-box', html, re.IGNORECASE)
    if not match:
        match = re.search(r'<div[^>]*id="article_content"[^>]*>([\s\S]+?)</div>\s*(?=<div[^>]*class="[^"]*recommend|<div[^>]*id="[^"]*comment)', html, re.IGNORECASE)

    if match:
        content = match.group(1)
        # ç§»é™¤"é˜…è¯»æ›´å¤š"é®ç½©
        content = re.sub(r'<div[^>]*class="[^"]*hide-article-box[^"]*"[^>]*>[\s\S]*?</div>', '', content, flags=re.IGNORECASE)
        return content

    return html


def _extract_weibo_content(html: str) -> str:
    """ä»å¾®åšé¡µé¢æå–æ­£æ–‡å’Œäº’åŠ¨æ•°æ®"""
    results = []

    # æå–å¾®åšå¡ç‰‡
    cards = re.split(r'<div[^>]*class="[^"]*card-wrap[^"]*"', html)

    for card in cards[1:]:
        # æå–ç”¨æˆ·å
        user_match = re.search(r'nick-name="([^"]+)"', card)
        user = user_match.group(1) if user_match else ''

        # æå–æ­£æ–‡
        text_match = re.search(r'<p[^>]*class="[^"]*txt[^"]*"[^>]*>([\s\S]+?)</p>', card)
        if not text_match:
            continue
        text = text_match.group(1).strip()

        # æå–äº’åŠ¨æ•°æ®
        stats = []
        # è½¬å‘
        repost_match = re.search(r'è½¬å‘\s*(\d+)', card)
        if repost_match and repost_match.group(1) != '0':
            stats.append(f"ğŸ”„ {repost_match.group(1)}")
        # è¯„è®º
        comment_match = re.search(r'è¯„è®º\s*(\d+)', card)
        if comment_match and comment_match.group(1) != '0':
            stats.append(f"ğŸ’¬ {comment_match.group(1)}")
        # ç‚¹èµ
        like_match = re.search(r'èµ\s*(\d+)', card)
        if like_match and like_match.group(1) != '0':
            stats.append(f"ğŸ‘ {like_match.group(1)}")

        stats_line = f'<p><em>{" | ".join(stats)}</em></p>' if stats else ''

        content = f'<p><strong>@{user}</strong></p>\n{text}\n{stats_line}' if user else text
        results.append(content)

    if results:
        return '<hr>\n'.join(results)

    # å›é€€ï¼šç®€å•æå–
    return _simple_extract(html)


def _extract_zhihu_from_dom(html: str) -> str:
    """ä»çŸ¥ä¹é¡µé¢DOMæå–æ­£æ–‡ï¼ˆæµè§ˆå™¨æ¨¡å¼ï¼Œæ‡’åŠ è½½åçš„å†…å®¹ï¼‰"""
    results = []

    # ç”¨åˆ†å‰²æ³•ï¼šæŒ‰AnswerItemåˆ†å‰²ï¼Œæ¯å—æå–ä½œè€…å’Œå†…å®¹
    parts = re.split(r'<div[^>]*class="[^"]*AnswerItem[^"]*"', html)

    for part in parts[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆAnswerItemä¹‹å‰çš„å†…å®¹ï¼‰
        # æå–ä½œè€…å
        author_match = re.search(r'class="[^"]*UserLink-link[^"]*"[^>]*>([^<]+)</a>', part)
        author = author_match.group(1).strip() if author_match else ''

        # æå–äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€è¯„è®ºï¼‰
        stats = []
        vote_match = re.search(r'button[^>]*VoteButton[^>]*>([^<]*\d+[^<]*)</button>', part)
        if vote_match:
            stats.append(f"ğŸ‘ {vote_match.group(1).strip()}")
        comment_match = re.search(r'>(\d+)\s*æ¡è¯„è®º<', part)
        if comment_match:
            stats.append(f"ğŸ’¬ {comment_match.group(1)}")
        stats_line = f'<p><em>{" | ".join(stats)}</em></p>\n' if stats else ''

        # æå–å†…å®¹ï¼ˆRichContent-inneråˆ°ä¸‹ä¸€ä¸ªä¸»è¦divç»“æŸï¼‰
        content_match = re.search(r'<div[^>]*class="[^"]*RichContent-inner[^"]*"[^>]*>([\s\S]+?)<div[^>]*class="[^"]*ContentItem-actions', part)
        if content_match:
            content = content_match.group(1).strip()
            if author:
                content = f'<p><strong>ç­”ä¸»ï¼š{author}</strong></p>\n' + stats_line + content
            results.append(content)

    if results:
        return '<hr>\n'.join(results)
    return ''


def _extract_zhihu_content(html: str) -> str:
    """ä»çŸ¥ä¹é¡µé¢æå–æ­£æ–‡ï¼ˆä»å†…åµŒJSONä¸­ï¼‰"""
    import json
    match = re.search(r'<script[^>]*id="js-initialData"[^>]*>([^<]+)</script>', html)
    if match:
        try:
            data = json.loads(match.group(1))
            entities = data.get('initialState', {}).get('entities', {})
            users = entities.get('users', {})
            results = []

            # å°è¯•ä¸“æ æ–‡ç« 
            articles = entities.get('articles', {})
            for article in articles.values():
                content = article.get('content', '')
                if content:
                    author_id = article.get('author', '')
                    author = users.get(author_id, {})
                    author_name = author.get('name', '')
                    # äº’åŠ¨æ•°æ®
                    stats = []
                    if article.get('voteupCount'):
                        stats.append(f"ğŸ‘ {article['voteupCount']}")
                    if article.get('commentCount'):
                        stats.append(f"ğŸ’¬ {article['commentCount']}")
                    stats_line = f'<p><em>{" | ".join(stats)}</em></p>\n' if stats else ''
                    if author_name:
                        content = f'<p><strong>ä½œè€…ï¼š{author_name}</strong></p>\n' + stats_line + content
                    results.append(content)

            # å°è¯•é—®ç­”ï¼ˆæ”¶é›†æ‰€æœ‰å›ç­”ï¼‰
            answers = entities.get('answers', {})
            for answer in answers.values():
                content = answer.get('content', '')
                if content:
                    author_info = answer.get('author', {})
                    author_name = author_info.get('name', '') if isinstance(author_info, dict) else users.get(author_info, {}).get('name', '')
                    # äº’åŠ¨æ•°æ®
                    stats = []
                    if answer.get('voteupCount'):
                        stats.append(f"ğŸ‘ {answer['voteupCount']}")
                    if answer.get('commentCount'):
                        stats.append(f"ğŸ’¬ {answer['commentCount']}")
                    stats_line = f'<p><em>{" | ".join(stats)}</em></p>\n' if stats else ''
                    if author_name:
                        content = f'<p><strong>ç­”ä¸»ï¼š{author_name}</strong></p>\n' + stats_line + content
                    results.append(content)

            if results:
                return '<hr>\n'.join(results)
        except Exception:
            pass
    return ''


def _simple_extract(html: str) -> str:
    """ç®€å•æå–æ­£æ–‡ï¼ˆå½“ readability ä¸å¯ç”¨æ—¶ï¼‰"""
    # ç§»é™¤ script/style/æ³¨é‡Š
    html = re.sub(r'<script[^>]*>[\s\S]*?</script>', '', html, flags=re.IGNORECASE)
    html = re.sub(r'<style[^>]*>[\s\S]*?</style>', '', html, flags=re.IGNORECASE)
    html = re.sub(r'<!--[\s\S]*?-->', '', html)
    # ç§»é™¤å¯¼èˆª/é¡µçœ‰/é¡µè„š/ä¾§è¾¹æ /å¹¿å‘Š/noscript
    for tag in ['nav', 'header', 'footer', 'aside', 'iframe', 'noscript']:
        html = re.sub(rf'<{tag}[^>]*>[\s\S]*?</{tag}>', '', html, flags=re.IGNORECASE)
    # ç§»é™¤å¸¸è§æ— å…³class/idçš„div
    noise_kw = r'nav|menu|sidebar|footer|header|comment|recommend|related|ad|share|social'
    noise_kw += r'|logo|copyright|qrcode|äºŒç»´ç |ç‰ˆæƒ|åˆ†äº«|è¯„è®º|æ¨è|ç›¸å…³|çƒ­æ¦œ|çƒ­æœ|trending|search'
    pattern = rf'<div[^>]*(?:class|id)=["\'][^"\']*(?:{noise_kw})[^"\']*["\'][^>]*>[\s\S]*?</div>'
    html = re.sub(pattern, '', html, flags=re.IGNORECASE)

    # å°è¯•æå– article æˆ– main æˆ–ç‰¹å®šclass
    content_classes = r'post-content|article-content|entry-content|content-body'
    content_classes += r'|post_body|post_text|article_body|news_body|main-content'
    content_classes += r'|RichContent|Post-RichText|AnswerItem|ztext'  # çŸ¥ä¹
    for pattern in [
        r'<article[^>]*>([\s\S]*?)</article>',
        r'<main[^>]*>([\s\S]*?)</main>',
        rf'<div[^>]*class=["\'][^"\']*(?:{content_classes})[^"\']*["\'][^>]*>([\s\S]*?)</div>',
    ]:
        match = re.search(pattern, html, re.IGNORECASE)
        if match:
            return match.group(0)

    return html


def _filter_noise_images(html: str) -> str:
    """è¿‡æ»¤å™ªéŸ³å›¾ç‰‡ï¼ˆlogoã€äºŒç»´ç ã€å°å›¾æ ‡ã€SVGå ä½å›¾ç­‰ï¼‰"""
    def should_remove(match):
        img_tag = match.group(0)
        # è¿‡æ»¤SVGå ä½å›¾
        if 'data:image/svg+xml' in img_tag:
            return ''
        # æ£€æŸ¥æ˜¯å¦æ˜¯logo/icon/qrcodeç­‰
        if re.search(r'(?:logo|icon|qrcode|äºŒç»´ç |badge|avatar)', img_tag, re.IGNORECASE):
            return ''
        # æ£€æŸ¥å°ºå¯¸ï¼Œè¿‡æ»¤å°å›¾ç‰‡ï¼ˆå®½æˆ–é«˜<100ï¼‰
        size_match = re.search(r'(?:width|height)[=:]\s*["\']?(\d+)', img_tag, re.IGNORECASE)
        if size_match and int(size_match.group(1)) < 100:
            return ''
        return img_tag

    return re.sub(r'<img[^>]*>', should_remove, html, flags=re.IGNORECASE)
